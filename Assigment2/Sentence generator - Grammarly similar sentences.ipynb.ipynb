{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Charakterystka gramatyczna.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j19r7M-JuYd1"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnaJZ_4rvnhb"
      },
      "source": [
        "def load_supertags(path):\n",
        "\n",
        "    file = open(path, 'r')\n",
        "    word_to_tag = {}\n",
        "\n",
        "    for line in tqdm(file):\n",
        "\n",
        "        word, tag = line.split(\" \")\n",
        "        if tag[-1] == \"\\n\": tag = tag[:-1]\n",
        "\n",
        "        word_to_tag[word.lower()] = tag\n",
        "    \n",
        "    file.close()\n",
        "    return word_to_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ4O5-F7ZQWN"
      },
      "source": [
        "# Unigrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrKVjzXlcyCB"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0NSPEQ62Bzu"
      },
      "source": [
        "def load_unigrams(path):\n",
        "\n",
        "    file = open(path, 'r')\n",
        "    word_to_occurrence = {}\n",
        "\n",
        "    for line in tqdm(file):\n",
        "\n",
        "        words = line.lower().split(\" \")\n",
        "        \n",
        "        for word in words:\n",
        "\n",
        "            if word[-1] == \"\\n\": word = word[:-1]\n",
        "\n",
        "            if word in word_to_occurrence.keys():\n",
        "                word_to_occurrence[word] += 1\n",
        "            else:\n",
        "                word_to_occurrence[word] = 1\n",
        "\n",
        "    return word_to_occurrence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2Bj7c0S3FAI"
      },
      "source": [
        "def update_tags_and_create_tag_to_wo(word_to_tag, word_to_occurrence):\n",
        "\n",
        "    tag_to_words_with_occurrence = {}\n",
        "\n",
        "    for word in tqdm(word_to_occurrence.keys()):\n",
        "\n",
        "        if word not in word_to_tag.keys():\n",
        "            word_to_tag[word] = '^' + word\n",
        "\n",
        "        tag = word_to_tag[word]\n",
        "        \n",
        "        if tag in tag_to_words_with_occurrence.keys():\n",
        "            tag_to_words_with_occurrence[tag].append([word, word_to_occurrence[word]])\n",
        "        else:\n",
        "            tag_to_words_with_occurrence[tag] = [[word, word_to_occurrence[word]]]\n",
        "    \n",
        "    return tag_to_words_with_occurrence, word_to_tag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exrHBbD67OLV"
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "def choose_word_softmax(words):\n",
        "\n",
        "    occurrences = np.array([word[1] for word in words], dtype=np.float64)\n",
        "\n",
        "    occurrences = (occurrences - min(occurrences)) / (max(occurrences) - min(occurrences) + 0.0001)\n",
        "\n",
        "    number_of_words = len(words)\n",
        "    probs = softmax(occurrences)\n",
        "    word_idx = np.random.choice(number_of_words , size = 1, p = probs)[0]\n",
        "\n",
        "    return words[word_idx][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJvItDI78QgQ"
      },
      "source": [
        "def find_grammatically_similar_sentence_unigram(sen, word_to_tag, tag_to_words_with_occurrence):\n",
        "\n",
        "    res = []\n",
        "\n",
        "    for word in sen:\n",
        "\n",
        "        if word in word_to_tag.keys():\n",
        "            tag = word_to_tag[word]\n",
        "        else:\n",
        "            return \"error :\" + word\n",
        "\n",
        "        res.append(choose_word_softmax(tag_to_words_with_occurrence[tag]))\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJL7ihtKwIgL",
        "outputId": "fa49d8e3-4049-42ce-f543-ae2b050977c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path_supertags = '/content/drive/My Drive/Colab Notebooks/NLP/Dane/Copy of supertags.txt'\n",
        "path_corpus = '/content/polish_corpora.txt'\n",
        "\n",
        "word_to_tag = load_supertags(path_supertags)\n",
        "word_to_occurrence = load_unigrams(path_corpus)\n",
        "tag_to_words_with_occurrence, word_to_tag = update_tags_and_create_tag_to_wo(word_to_tag, word_to_occurrence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1781994it [00:02, 626084.89it/s]\n",
            "23011601it [04:59, 76755.20it/s]\n",
            "100%|██████████| 3591114/3591114 [00:11<00:00, 319705.03it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD2eTRUT7-wD"
      },
      "source": [
        "sen1 = 'Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę'.lower().split(' ')\n",
        "sen2 = 'przed ćwiczeniami studenci dostarczają prowadzącemu deklaracje'.lower().split(' ')\n",
        "sen3 = 'rycerze osaczyli smoka zanim strawił księżniczkę'.lower().split(' ')\n",
        "sen4 = 'Przed wyruszeniem w drogę należy zebrać drużynę'.lower().split(' ')\n",
        "sen5 = 'W zadaniu tym powinieneś losować zdania o słowach z identyczną charakterystyką gramatyczną jak zdanie wejściowe'.lower().split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1xEPmuy-RAs",
        "outputId": "1170fbe6-0902-49ef-81a0-a554c4f6c2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen1))\n",
        "print(\" \".join(find_grammatically_similar_sentence_unigram(sen1, word_to_tag, tag_to_words_with_occurrence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mały piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę\n",
            "metalograficzny haszek sprofesjonalizował w nieprzejezdnej metrówce zbieracko rozpisaną lubekę\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoUBfzPneFaf",
        "outputId": "18cd4c9b-52d2-4b34-b5c0-30bb18cad86b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen2))\n",
        "print(\" \".join(find_grammatically_similar_sentence_unigram(sen2, word_to_tag, tag_to_words_with_occurrence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "przed ćwiczeniami studenci dostarczają prowadzącemu deklaracje\n",
            "w usunięciami okupanci gotują wyświetlającemu wyobraźnie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13ZY0BZteFoa",
        "outputId": "4b5eb4db-674e-4a73-f4e3-429ccc69a633",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen3))\n",
        "print(\" \".join(find_grammatically_similar_sentence_unigram(sen3, word_to_tag, tag_to_words_with_occurrence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rycerze osaczyli smoka zanim strawił księżniczkę\n",
            "nakładacze podciągnęli kocurka aniżeli liznął anastrofę\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5EgFlw4eF18",
        "outputId": "ef46ed56-d9d0-447e-de5b-0c594b0b5ad1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen4))\n",
        "print(\" \".join(find_grammatically_similar_sentence_unigram(sen4, word_to_tag, tag_to_words_with_occurrence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "przed wyruszeniem w drogę należy zebrać drużynę\n",
            "przed schodzeniem ponad uchę asystuje uprościć fascjolozę\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdX4oPmJeGBR",
        "outputId": "2a8d80cd-839a-4a39-ec1c-212ec939c5c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen5))\n",
        "print(\" \".join(find_grammatically_similar_sentence_unigram(sen5, word_to_tag, tag_to_words_with_occurrence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w zadaniu tym powinieneś losować zdania o słowach z identyczną charakterystyką gramatyczną jak zdanie wejściowe\n",
            "w wyklepaniu owym powinieneś holografować sczyszczenia o przezroczach ze kwaskowatą obrzędowością attykową jak upominanie frywolne\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXJCCHv-ZR6g"
      },
      "source": [
        "# Bigrams + Unigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRF3ME07gVZp"
      },
      "source": [
        "def load_bigrams_and_create_bitags_to_bigram_occurrence(path, word_to_tag, amount):\n",
        "    \n",
        "    file = open(path, 'r')\n",
        "    bigramtag_to_bigram_occurrence = {}\n",
        "\n",
        "    for iter, line in tqdm(enumerate(file)):\n",
        "\n",
        "        oc, word1, word2 = line.lower().split(\" \")\n",
        "\n",
        "        if word2[-1] == \"\\n\": word2 = word2[:-1]\n",
        "\n",
        "        if word1 not in word_to_tag.keys() or word2 not in word_to_tag.keys(): continue\n",
        "    \n",
        "        if iter == amount: break\n",
        "\n",
        "        key = (word_to_tag[word1], word_to_tag[word2])\n",
        "\n",
        "        if key in bigramtag_to_bigram_occurrence.keys():\n",
        "            bigramtag_to_bigram_occurrence[key].append(((word1, word2), int(oc)))\n",
        "        else:\n",
        "            bigramtag_to_bigram_occurrence[key] = [((word1, word2), int(oc))]     \n",
        "\n",
        "\n",
        "    return bigramtag_to_bigram_occurrence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIgSdCGmZUoY",
        "outputId": "868accda-5782-4cde-d6ce-780a44787471",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bigrams_path = '/content/poleval_2grams.txt'\n",
        "bitag_to_bigram_occurence = load_bigrams_and_create_bitags_to_bigram_occurrence(bigrams_path, word_to_tag, 24000000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23982821it [02:01, 222553.78it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEdXGyakhteY"
      },
      "source": [
        "def find_grammatically_similar_sentence_bigram(sen, word_to_tag, bitag_to_bigram_occurence, tag_to_words_with_occurrence, recall):\n",
        "\n",
        "    res = []\n",
        "    \n",
        "    err = 0\n",
        "    while len(res) != len(sen):\n",
        "\n",
        "        if len(res) < 2:\n",
        "            res = []\n",
        "            err+= 1\n",
        "            ### Generate first bigram\n",
        "            word_1 = sen[0]\n",
        "            word_2 = sen[1]\n",
        "\n",
        "            if word_1 not in word_to_tag or word_2 not in word_to_tag: return \"No tag for word\"\n",
        "            \n",
        "            key = (word_to_tag[word_1], word_to_tag[word_2])\n",
        "            \n",
        "            pred_1, pred_2 = choose_word_softmax(bitag_to_bigram_occurence[key])\n",
        "            res.append(pred_1)\n",
        "            res.append(pred_2)\n",
        "\n",
        "        len_res = len(res) - 1\n",
        "            \n",
        "        word_1 = sen[len_res]\n",
        "        word_2 = sen[len_res+1]\n",
        "\n",
        "        match_word = res[-1]\n",
        "        if match_word[0] == '|': match_word = match_word[1:]\n",
        "\n",
        "        if word_1 not in word_to_tag or word_2 not in word_to_tag: return \"No tag for word\"\n",
        "\n",
        "        if err == recall:   \n",
        "            res.append('|' + choose_word_softmax(tag_to_words_with_occurrence[word_to_tag[word_2]]))\n",
        "            \n",
        "            err = 0\n",
        "            continue\n",
        "\n",
        "        key = (word_to_tag[word_1], word_to_tag[word_2])\n",
        "\n",
        "        words = [((bigram[0], bigram[1]), int(oc)) for bigram, oc in bitag_to_bigram_occurence[key] if bigram[0]==match_word]\n",
        "\n",
        "        if len(words) == 0:\n",
        "            res = res[:-1]\n",
        "            err += 1\n",
        "        else:\n",
        "            pred_1, pred_2 = choose_word_softmax(words)\n",
        "            res.append(pred_2)\n",
        "    \n",
        "    return res   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SskmnH0DKOf"
      },
      "source": [
        "recall = 30\n",
        "sen1 = 'Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę'.lower().split(' ')\n",
        "sen2 = 'przed ćwiczeniami studenci dostarczają prowadzącemu deklaracje'.lower().split(' ')\n",
        "sen3 = 'rycerze osaczyli smoka zanim strawił księżniczkę'.lower().split(' ')\n",
        "sen4 = 'Przed wyruszeniem w drogę należy zebrać drużynę'.lower().split(' ')\n",
        "sen5 = 'W zadaniu tym powinieneś losować zdania o słowach z identyczną charakterystyką gramatyczną jak zdanie wejściowe'.lower().split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndRE5YDoDLxV",
        "outputId": "2b50e09c-6c2e-4c2b-c657-5ad453d37de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen1))\n",
        "print(\" \".join(find_grammatically_similar_sentence_bigram(sen1, word_to_tag, bitag_to_bigram_occurence, tag_to_words_with_occurrence, recall)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mały piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę\n",
            "godny następca podpisał w nietuzinkowej architekturze bardzo skrywaną tajemnicę\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkR6CTHiDORy",
        "outputId": "7f7fc5a8-de37-4110-c84c-ed88437d31f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen2))\n",
        "print(\" \".join(find_grammatically_similar_sentence_bigram(sen2, word_to_tag, bitag_to_bigram_occurence, tag_to_words_with_occurrence, recall)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "przed ćwiczeniami studenci dostarczają prowadzącemu deklaracje\n",
            "nad ustaleniami członkowie składają |świadczącemu prace\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJFW8LmzDOcu",
        "outputId": "bff518a3-2280-434f-b025-61022c9b1522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen3))\n",
        "print(\" \".join(find_grammatically_similar_sentence_bigram(sen3, word_to_tag, bitag_to_bigram_occurence, tag_to_words_with_occurrence, recall)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rycerze osaczyli smoka zanim strawił księżniczkę\n",
            "obywatele stracili sokoła zanim spotkał ekspedycję\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYLihHN1DOnS",
        "outputId": "4dd6a8af-e1f3-464f-8fbc-42c455031e59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen4))\n",
        "print(\" \".join(find_grammatically_similar_sentence_bigram(sen4, word_to_tag, bitag_to_bigram_occurence, tag_to_words_with_occurrence, recall)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "przed wyruszeniem w drogę należy zebrać drużynę\n",
            "pomiędzy odżywianiem przed bramkę należy dofinansować różnicę\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9EmP4q9DO1b",
        "outputId": "d49b2b97-7773-4018-e70d-4331f733a964",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\" \".join(sen5))\n",
        "print(\" \".join(find_grammatically_similar_sentence_bigram(sen5, word_to_tag, bitag_to_bigram_occurence, tag_to_words_with_occurrence, recall)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w zadaniu tym powinieneś losować zdania o słowach z identyczną charakterystyką gramatyczną jak zdanie wejściowe\n",
            "na pomieszczeniu |którym powinieneś spodziewać rozpoczęcia o miejscach z godzinną normą równą jak zabezpieczanie antykorozyjne\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}