{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenizer - BPE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PtN05TGTmIf"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "import numpy as np\r\n",
        "import re, collections\r\n",
        "import pickle\r\n",
        "from multiprocessing import Pool, Process\r\n",
        "import multiprocessing\r\n",
        "from functools import partial\r\n",
        "from math import log"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD5NP5jVDqwm",
        "outputId": "9307a351-9dc8-41a0-cb41-43e8a4a9e776"
      },
      "source": [
        "!pip install cython\r\n",
        "%load_ext Cython"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (0.29.21)\n",
            "The Cython extension is already loaded. To reload it, use:\n",
            "  %reload_ext Cython\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bbm53OlDyv9"
      },
      "source": [
        "a_file = open(\"/content/drive/MyDrive/74.pkl\", \"rb\")\r\n",
        "vocab = pickle.load(a_file)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04Pbi_9KLe-P"
      },
      "source": [
        "%%cython\r\n",
        "\r\n",
        "def get_vocab(filename):\r\n",
        "    \r\n",
        "    vocab = {}\r\n",
        "    \r\n",
        "    for line in open(filename, 'r', encoding='utf-8'):\r\n",
        "\r\n",
        "        line = line.strip()\r\n",
        "\r\n",
        "        key = ' '.join(list(line))\r\n",
        "\r\n",
        "        if key in vocab.keys():\r\n",
        "            vocab[key] += 1\r\n",
        "        else:\r\n",
        "            vocab[key] = 1\r\n",
        "\r\n",
        "    return vocab\r\n",
        "\r\n",
        "def get_freq(vocab, max_len):\r\n",
        "\r\n",
        "    freqs = {}\r\n",
        "    symbol_freqs = {}\r\n",
        "    all = 0\r\n",
        "    all_2 = 0\r\n",
        "\r\n",
        "\r\n",
        "    for line, freq in vocab.items():\r\n",
        "\r\n",
        "        symbols = line.split()\r\n",
        "\r\n",
        "        all += len(symbols)\r\n",
        "\r\n",
        "        for symbol_i in range(len(symbols)-1):\r\n",
        "\r\n",
        "            key = symbols[symbol_i]\r\n",
        "            if key in symbol_freqs.keys():\r\n",
        "                symbol_freqs[key] += freq\r\n",
        "            else:\r\n",
        "                symbol_freqs[key] = freq\r\n",
        "\r\n",
        "\r\n",
        "            if max_len >= len(symbols[symbol_i] + symbols[symbol_i+1]):\r\n",
        "                all_2 += freq\r\n",
        "                \r\n",
        "                key = (symbols[symbol_i], symbols[symbol_i+1])\r\n",
        "                if key in freqs.keys():\r\n",
        "                    freqs[key] += freq\r\n",
        "                else:\r\n",
        "                    freqs[key] = freq\r\n",
        "\r\n",
        "            key = symbols[-1]\r\n",
        "            if key in symbol_freqs.keys():\r\n",
        "                symbol_freqs[key] += freq\r\n",
        "            else:\r\n",
        "                symbol_freqs[key] = freq\r\n",
        "\r\n",
        "    for key in freqs.keys():\r\n",
        "        freqs[key] = (freqs[key]/all_2) / ((symbol_freqs[key[0]]/all)  * (symbol_freqs[key[1]]/all))\r\n",
        "    \r\n",
        "    return freqs\r\n",
        "\r\n",
        "\r\n",
        "def merge_vocab(pair, v_in):\r\n",
        "\r\n",
        "    v_out = {}\r\n",
        "\r\n",
        "    bigram = ''.join(pair)\r\n",
        "    pair = pair[0] + ' ' + pair[1]\r\n",
        "\r\n",
        "    for line in v_in:\r\n",
        "\r\n",
        "        w_out = line.replace(pair,bigram)\r\n",
        "        v_out[w_out] = v_in[line]\r\n",
        "\r\n",
        "    return v_out\r\n",
        "    \r\n",
        "def get_tokens(vocab):\r\n",
        "\r\n",
        "    tokens = set({})\r\n",
        "\r\n",
        "    for line in vocab.keys():\r\n",
        "        line = line.split()\r\n",
        "\r\n",
        "        for token in line:\r\n",
        "            tokens.add(token)\r\n",
        "    \r\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekcoz6IZGywe"
      },
      "source": [
        "vocab = get_vocab('/content/sentences_for_task1.txt')\r\n",
        "\r\n",
        "max_len = 16\r\n",
        "num_merges = 15000\r\n",
        "\r\n",
        "t = tqdm(np.arange(num_merges))\r\n",
        "for i in t:\r\n",
        "\r\n",
        "    pairs = get_freq(vocab, max_len)\r\n",
        "    if len(pairs) == 0: break\r\n",
        "    if max(pairs.values()) < 2: break\r\n",
        "    \r\n",
        "    pair = max(pairs, key=pairs.get)\r\n",
        "    \r\n",
        "\r\n",
        "    vocab = merge_vocab(pair, vocab)\r\n",
        "\r\n",
        "    if i % 1000 == 0:\r\n",
        "        tokens = get_tokens(vocab)\r\n",
        "        mt = np.mean([len(token) for token in tokens])\r\n",
        "\r\n",
        "        t.set_postfix({'mean_token_len': mt, 'vocab_size': len(tokens)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lxykM37sEOo"
      },
      "source": [
        " tokens = get_tokens(vocab)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvfcOXC4qIzK",
        "outputId": "141bdd27-427f-43a8-8d08-3b665f0a5df0"
      },
      "source": [
        "np.mean([len(token) for token in tokens])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.901111280457501"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf8prLoIH3pt",
        "outputId": "6fc98163-0953-42fa-e0cb-cc876059d190"
      },
      "source": [
        "score = 0\r\n",
        "\r\n",
        "for i, line in enumerate(open('/content/test_for_task1.txt', 'r')):\r\n",
        "    w1,w2 = line.split()\r\n",
        "\r\n",
        "    if w1 in tokens and w2 not in tokens:\r\n",
        "        score+=1\r\n",
        "    elif w1 not in tokens and w2 in tokens:\r\n",
        "        score+=0\r\n",
        "    else:\r\n",
        "        score+=0.5\r\n",
        "\r\n",
        "print(score/i)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7421974219742198\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vunaqthkr1KL"
      },
      "source": [
        "aweaimu a_file = open(\"data.pkl\", \"wb\")\r\n",
        "pickle.dump(vocab, a_file)\r\n",
        "a_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}